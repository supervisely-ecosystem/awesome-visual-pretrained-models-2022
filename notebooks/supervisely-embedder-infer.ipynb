{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install timm -q","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-12-14T15:27:55.142657Z","iopub.execute_input":"2022-12-14T15:27:55.143089Z","iopub.status.idle":"2022-12-14T15:27:55.148804Z","shell.execute_reply.started":"2022-12-14T15:27:55.143052Z","shell.execute_reply":"2022-12-14T15:27:55.147193Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# import timm\nimport torch\nimport numpy as np\nimport os, sys, shutil\nimport transformers\nimport requests\nfrom PIL import Image\nfrom pathlib import Path\nfrom scipy.io import loadmat\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm.auto import tqdm\n","metadata":{"execution":{"iopub.status.busy":"2022-12-14T15:27:55.157408Z","iopub.execute_input":"2022-12-14T15:27:55.157703Z","iopub.status.idle":"2022-12-14T15:27:57.063305Z","shell.execute_reply.started":"2022-12-14T15:27:55.157675Z","shell.execute_reply":"2022-12-14T15:27:57.062297Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import torchvision\nfrom matplotlib import pyplot as plt\n\ndef show_tensor(tensor, transpose=None, normalize=None, figsize=(10,10), nrow=None, padding=2, verbose=True, **kwargs):\n    '''Flexible tool for visulizing tensors of any shape. Support batch_size >= 1.'''\n    if not isinstance(tensor, torch.Tensor):\n        tensor = torch.tensor(np.array(tensor))\n    tensor = tensor.detach().cpu().float()\n    \n    if tensor.ndim == 4 and tensor.shape[1] == 1:\n        if verbose: print('processing as black&white')\n        tensor = tensor.repeat(1,3,1,1)\n    elif tensor.ndim == 3:\n        tensor = tensor.unsqueeze(0)\n    elif tensor.ndim == 2:\n        if verbose: print('processing as black&white')\n        tensor = tensor.unsqueeze(0).repeat(3,1,1).unsqueeze(0)\n        \n    if normalize is None:\n        if tensor.max() <= 1.0 and tensor.min() >= 0.0:\n            normalize = False\n        else:\n            if verbose: print('tensor has been normalized to [0., 1.]')\n            normalize = True\n            \n    if transpose is None:\n        transpose = True if tensor.shape[1] != 3 else False\n    if transpose:\n        tensor = tensor.permute(0,3,1,2)\n    \n    if nrow is None:\n        nrow = int(np.ceil(np.sqrt(tensor.shape[0])))\n        \n    grid = torchvision.utils.make_grid(tensor, normalize=normalize, nrow=nrow, padding=padding, **kwargs)\n    plt.figure(figsize=figsize)\n    return plt.imshow(grid.permute(1,2,0))","metadata":{"execution":{"iopub.status.busy":"2022-12-14T15:27:57.065665Z","iopub.execute_input":"2022-12-14T15:27:57.066358Z","iopub.status.idle":"2022-12-14T15:27:57.286935Z","shell.execute_reply.started":"2022-12-14T15:27:57.066313Z","shell.execute_reply":"2022-12-14T15:27:57.285981Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class CarsDataset(torch.utils.data.Dataset):\n    def __init__(self, train_path, train_targets):\n        self.train_path = train_path\n        self.files = list(map(str, Path(train_path).glob('*.*')))\n        self.train_targets = train_targets\n        \n    def __getitem__(self, idx):\n        img = Image.open(self.files[idx]).convert('RGB')\n        return img, train_targets[idx]\n    \n    def __len__(self):\n        return len(self.files)\n    \n    \ndef get_features(model, processor, images, pool_mode='auto'):\n    inputs = processor(images=images, return_tensors=\"pt\", padding=True)\n    inputs = {k:x.to(device) for k,x in inputs.items()}\n\n    with torch.no_grad():\n        if hasattr(model, 'get_image_features'):\n            # for CLIP, FLAVA\n            out = model.get_image_features(**inputs)\n        else:\n            # for ConvNeXt, BEiT\n            out_raw = model(**inputs)\n            if pool_mode == 'auto':\n                out = out_raw.pooler_output\n            elif pool_mode == 'mean':\n                out = out_raw.last_hidden_state.mean(1)\n        if out.ndim == 3:\n            out = out.mean(1)\n\n    return out\n\n    \ndef get_all_features(loader, model, processor, debug=False, first_for_debug=True):\n    if first_for_debug:\n        batch = next(iter(loader))\n        imgs, targets = zip(*batch)\n        out = get_features(model, processor, imgs)\n        print(out.shape)\n        \n    all_features = []\n\n    for batch in tqdm(loader):\n        imgs, targets = zip(*batch)\n        out = get_features(model, processor, imgs)\n        all_features.append(out)\n        if debug: break\n\n    all_features = torch.cat(all_features, 0)\n    \n    return all_features","metadata":{"execution":{"iopub.status.busy":"2022-12-14T15:33:46.978430Z","iopub.execute_input":"2022-12-14T15:33:46.978869Z","iopub.status.idle":"2022-12-14T15:33:46.992693Z","shell.execute_reply.started":"2022-12-14T15:33:46.978833Z","shell.execute_reply":"2022-12-14T15:33:46.991727Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"with open(\"/kaggle/input/stanford-cars-dataset/car_devkit/devkit/train_perfect_preds.txt\") as f:\n    train_targets = np.array(list(map(lambda x: int(x), f.readlines())))\n\ncar_names = loadmat(\"/kaggle/input/stanford-cars-dataset/car_devkit/devkit/cars_meta.mat\")['class_names'][0]","metadata":{"execution":{"iopub.status.busy":"2022-12-14T15:27:57.304216Z","iopub.execute_input":"2022-12-14T15:27:57.305366Z","iopub.status.idle":"2022-12-14T15:27:57.342512Z","shell.execute_reply.started":"2022-12-14T15:27:57.305320Z","shell.execute_reply":"2022-12-14T15:27:57.341648Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"dataset = CarsDataset(\"/kaggle/input/stanford-cars-dataset/cars_train/cars_train\", train_targets)\nloader = DataLoader(dataset, shuffle=False, batch_size=8, collate_fn=lambda x: x, num_workers=2)","metadata":{"execution":{"iopub.status.busy":"2022-12-14T15:27:57.344046Z","iopub.execute_input":"2022-12-14T15:27:57.344730Z","iopub.status.idle":"2022-12-14T15:27:57.494503Z","shell.execute_reply.started":"2022-12-14T15:27:57.344692Z","shell.execute_reply":"2022-12-14T15:27:57.493497Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"device = 'cuda'\n\nmodel_names = [\"openai/clip-vit-base-patch32\", \"facebook/flava-full\", \"facebook/convnext-xlarge-224-22k\", \"microsoft/beit-large-patch16-224-pt22k\"]\n# model_name = \"openai/clip-vit-base-patch32\"\nmodel_name = \"facebook/flava-full\"\n# model_name = \"facebook/convnext-xlarge-224-22k\"\n# model_name = \"microsoft/beit-large-patch16-224-pt22k\"\n\npool_mode = 'auto' # or 'mean'","metadata":{"execution":{"iopub.status.busy":"2022-12-14T15:42:50.692166Z","iopub.execute_input":"2022-12-14T15:42:50.693703Z","iopub.status.idle":"2022-12-14T15:42:50.701575Z","shell.execute_reply.started":"2022-12-14T15:42:50.693650Z","shell.execute_reply":"2022-12-14T15:42:50.700277Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"model = transformers.AutoModel.from_pretrained(model_name).to(device).eval()\nprocessor = transformers.AutoFeatureExtractor.from_pretrained(model_name)\nprint(model_name, sum(map(torch.numel, model.parameters()))/1e6, 'M parameters')  ","metadata":{"execution":{"iopub.status.busy":"2022-12-14T15:28:17.649645Z","iopub.execute_input":"2022-12-14T15:28:17.650027Z","iopub.status.idle":"2022-12-14T15:28:34.169385Z","shell.execute_reply.started":"2022-12-14T15:28:17.649995Z","shell.execute_reply":"2022-12-14T15:28:34.168158Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at facebook/flava-full were not used when initializing FlavaModel: ['mlm_head.decoder.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_4.bias', 'image_codebook.blocks.output.conv.bias', 'mim_head.transform.LayerNorm.weight', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_2.weight', 'mmm_image_head.transform.dense.weight', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_3.weight', 'mim_head.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_4.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_3.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_4.weight', 'mmm_image_head.transform.LayerNorm.weight', 'mim_head.decoder.bias', 'mlm_head.transform.dense.weight', 'mmm_text_head.transform.LayerNorm.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_2.bias', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_4.bias', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_2.bias', 'mlm_head.transform.LayerNorm.weight', 'itm_head.pooler.dense.bias', 'mim_head.decoder.weight', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_3.bias', 'itm_head.seq_relationship.weight', 'image_codebook.blocks.output.conv.weight', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_2.bias', 'image_codebook.blocks.group_2.group.block_1.id_path.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_4.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_2.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_1.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_2.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_4.bias', 'image_codebook.blocks.input.weight', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_1.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_1.bias', 'image_codebook.blocks.input.bias', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_3.weight', 'mmm_text_head.decoder.weight', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_4.weight', 'itm_head.seq_relationship.bias', 'mlm_head.bias', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_4.weight', 'mmm_image_head.decoder.weight', 'mmm_image_head.bias', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_1.weight', 'mlm_head.transform.LayerNorm.bias', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_3.weight', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_2.weight', 'mmm_image_head.decoder.bias', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_2.weight', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_4.bias', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_1.weight', 'image_codebook.blocks.group_3.group.block_1.id_path.weight', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_3.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_4.weight', 'mim_head.transform.LayerNorm.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_1.weight', 'mim_head.transform.dense.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_2.bias', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_1.weight', 'image_codebook.blocks.group_2.group.block_1.id_path.weight', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_3.bias', 'image_codebook.blocks.group_2.group.block_2.res_path.path.conv_3.bias', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_1.bias', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_3.bias', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_2.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_3.weight', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_2.weight', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_1.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_2.weight', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_3.weight', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_3.bias', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_2.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_4.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_1.weight', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_1.bias', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_3.weight', 'mmm_image_head.transform.dense.bias', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_1.weight', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_2.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_4.bias', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_4.weight', 'mim_head.transform.dense.weight', 'image_codebook.blocks.group_3.group.block_1.id_path.bias', 'mmm_text_head.transform.LayerNorm.bias', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_4.weight', 'image_codebook.blocks.group_1.group.block_2.res_path.path.conv_3.weight', 'mlm_head.decoder.bias', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_4.bias', 'image_codebook.blocks.group_3.group.block_1.res_path.path.conv_1.weight', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_2.bias', 'image_codebook.blocks.group_4.group.block_2.res_path.path.conv_1.bias', 'image_codebook.blocks.group_4.group.block_1.id_path.weight', 'mmm_text_head.decoder.bias', 'image_codebook.blocks.group_4.group.block_1.id_path.bias', 'image_codebook.blocks.group_1.group.block_1.res_path.path.conv_2.weight', 'mmm_image_head.transform.LayerNorm.bias', 'mmm_text_head.bias', 'mmm_text_head.transform.dense.weight', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_1.bias', 'itm_head.pooler.dense.weight', 'mmm_text_head.transform.dense.bias', 'image_codebook.blocks.group_2.group.block_1.res_path.path.conv_3.bias', 'image_codebook.blocks.group_3.group.block_2.res_path.path.conv_3.bias', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_1.bias', 'image_codebook.blocks.group_4.group.block_1.res_path.path.conv_4.weight', 'mlm_head.transform.dense.bias']\n- This IS expected if you are initializing FlavaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing FlavaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"name":"stdout","text":"facebook/flava-full 241.356289 M parameters\n","output_type":"stream"}]},{"cell_type":"code","source":"f = get_all_features(loader, model, processor, True)","metadata":{"execution":{"iopub.status.busy":"2022-12-14T15:33:50.737328Z","iopub.execute_input":"2022-12-14T15:33:50.737769Z","iopub.status.idle":"2022-12-14T15:33:52.111411Z","shell.execute_reply.started":"2022-12-14T15:33:50.737721Z","shell.execute_reply":"2022-12-14T15:33:52.109946Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"torch.Size([8, 768])\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1018 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c7cb5d5c6d54ee1bbb6c223704b93bc"}},"metadata":{}}]},{"cell_type":"code","source":"all_models_features = []\n\nfor model_name in model_names[-1:]:\n    model = transformers.AutoModel.from_pretrained(model_name).to(device).eval()\n    processor = transformers.AutoFeatureExtractor.from_pretrained(model_name)\n    print(model_name, sum(map(torch.numel, model.parameters()))/1e6, 'M parameters')    \n    \n    features = get_all_features(loader, model, processor)\n    torch.save(features, model_name.replace('/','_')+'.pt')\n#     all_models_features.append(features)","metadata":{"execution":{"iopub.status.busy":"2022-12-14T15:42:57.732251Z","iopub.execute_input":"2022-12-14T15:42:57.732644Z","iopub.status.idle":"2022-12-14T15:47:51.608819Z","shell.execute_reply.started":"2022-12-14T15:42:57.732613Z","shell.execute_reply":"2022-12-14T15:47:51.607371Z"},"trusted":true},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/737 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b76e59f6d77d4c57b8e2c76d2ba220ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.16G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2c2094df84e4723aad2a472bb3b5652"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/functional.py:568: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /usr/local/src/pytorch/aten/src/ATen/native/TensorShape.cpp:2227.)\n  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\nSome weights of the model checkpoint at microsoft/beit-large-patch16-224-pt22k were not used when initializing BeitModel: ['lm_head.bias', 'layernorm.weight', 'layernorm.bias', 'lm_head.weight']\n- This IS expected if you are initializing BeitModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BeitModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BeitModel were not initialized from the model checkpoint at microsoft/beit-large-patch16-224-pt22k and are newly initialized: ['beit.pooler.layernorm.weight', 'beit.pooler.layernorm.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/276 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93069d84d89540c98fccb22c07d5585c"}},"metadata":{}},{"name":"stdout","text":"microsoft/beit-large-patch16-224-pt22k 303.137216 M parameters\ntorch.Size([8, 1024])\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1018 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6f03451058f4a5c8541f6816130fbaa"}},"metadata":{}}]},{"cell_type":"code","source":"features.shape","metadata":{"execution":{"iopub.status.busy":"2022-12-14T15:53:00.406271Z","iopub.execute_input":"2022-12-14T15:53:00.407928Z","iopub.status.idle":"2022-12-14T15:53:00.418453Z","shell.execute_reply.started":"2022-12-14T15:53:00.407873Z","shell.execute_reply":"2022-12-14T15:53:00.417189Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"torch.Size([8144, 1024])"},"metadata":{}}]}]}